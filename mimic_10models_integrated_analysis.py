import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import shap
import warnings
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (accuracy_score, f1_score, recall_score,
                             confusion_matrix, roc_curve, roc_auc_score)

# ë¶€ë“œëŸ¬ìš´ ê³¡ì„  ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from scipy.interpolate import make_interp_spline

# ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# ì¶œë ¥ ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')


def run_comprehensive_analysis(input_file, exclude_cols, target_col,
                               run_external=False, external_file=None,
                               selected_models=None):
    """
    10ê°œ ëª¨ë¸ í†µí•© í•™ìŠµ, ê·¸ë¦¬ë“œ ì„œì¹˜, ë‚´ë¶€/ì™¸ë¶€ ê²€ì¦ ë° ì‹œê°í™” ìˆ˜í–‰
    """

    # 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    try:
        df = pd.read_csv(input_file)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {input_file} (ì´ {len(df)}í–‰)")
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return None, None, None

    X = df.drop(columns=[col for col in exclude_cols if col in df.columns] + [target_col])
    y = df[target_col]

    # ë‚´ë¶€ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  (8:2)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 2. ì „ì²´ ëª¨ë¸ í›„ë³´êµ° ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
    full_model_dict = {
        'LR': (LogisticRegression(max_iter=5000),
               {'clf__C': [1]}),
        'RF': (RandomForestClassifier(random_state=42),
               {'clf__n_estimators': [300], 'clf__max_depth': [20]}),
        'DT': (DecisionTreeClassifier(random_state=42),
               {'clf__max_depth': [5], 'clf__min_samples_leaf': [4]}),
        'KNN': (KNeighborsClassifier(),
                {'clf__n_neighbors': [21], 'clf__weights': ['distance']}),
        'MLP': (MLPClassifier(max_iter=1000, random_state=42),
                {'clf__hidden_layer_sizes': [(100,)], 'clf__alpha': [0.0001]}),
        'AdaBoost': (AdaBoostClassifier(random_state=42),
                     {'clf__n_estimators': [100], 'clf__learning_rate': [1.0]}),
        'SVM': (SVC(probability=True, random_state=42),
                {'clf__C': [1], 'clf__gamma': ['scale']}),
        'XGBoost': (XGBClassifier(random_state=42, eval_metric='logloss'),
                    {
                        'clf__n_estimators': [500],  # ë” ë§ì´ í•™ìŠµ
                        'clf__learning_rate': [0.01],  # í•™ìŠµë¥ ì„ ë‚®ì¶° ì •ë°€ë„ í–¥ìƒ
                        'clf__max_depth': [5],  # ë³µì¡ë„ë¥¼ ì‚´ì§ ì˜¬ë¦¼
                        'clf__min_child_weight': [5],  # ê³¼ì í•© ë°©ì§€ìš© (ì¤‘ìš”)
                        'clf__subsample': [0.8]  # ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨
                    }
                    ),
        'LightGBM': (LGBMClassifier(random_state=42),
                     {
                         'clf__n_estimators': [500],  # í˜„ì¬ 300ì´ ë¶€ì¡±í•´ ë³´ì„
                         'clf__learning_rate': [0.005],  # ë” ì˜ê²Œ ìª¼ê°œì„œ í•™ìŠµ
                         'clf__num_leaves': [63],  # íŠ¸ë¦¬ ë…¸ë“œ ê°œìˆ˜ë¥¼ ëŠ˜ë ¤ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
                         'clf__feature_fraction': [0.8],  # ë³€ìˆ˜ ìƒ˜í”Œë§ (ê³¼ì í•© ë°©ì§€)
                         'clf__min_child_samples': [30]  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ë°ì´í„° ìˆ˜
                     }
                     ),
        'CatBoost': (CatBoostClassifier(random_state=42, verbose=0),
                     {'clf__iterations': [300], 'clf__depth': [4], 'clf__learning_rate': [0.1]})
    }

    # ëª¨ë¸ í•„í„°ë§
    if selected_models is not None:
        model_dict = {k: v for k, v in full_model_dict.items() if k in selected_models}
        if not model_dict:
            print("âš ï¸ ì„ íƒëœ ëª¨ë¸ì´ ìœ íš¨í•˜ì§€ ì•Šì•„ ì „ì²´ ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            model_dict = full_model_dict
    else:
        model_dict = full_model_dict

    internal_results = []
    external_results = []
    trained_models = {}

    # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.figure(figsize=(12, 9))
    sns.set_style("whitegrid")
    # ëª¨ë¸ ê°œìˆ˜ì— ë§ëŠ” ìƒ‰ìƒ íŒ”ë ˆíŠ¸
    colors = sns.color_palette("husl", len(model_dict))

    print(f"ğŸ” [ë‚´ë¶€ í•™ìŠµ ì‹œì‘] ì´ {len(model_dict)}ê°œ ëª¨ë¸ ìµœì í™” ì¤‘...")

    for i, (name, (clf, params)) in enumerate(model_dict.items()):
        # Pipeline êµ¬ì¶•
        pipe = Pipeline([('scaler', StandardScaler()), ('clf', clf)])

        # Grid Search ì‹¤í–‰ (cv=3)
        grid = GridSearchCV(pipe, params, cv=10, n_jobs=-1, scoring='roc_auc')
        grid.fit(X_train, y_train)

        best_model = grid.best_estimator_
        trained_models[name] = best_model
        best_params = grid.best_params_

        print(f"âœ… {name} í•™ìŠµ ì™„ë£Œ! [Best: {best_params}]")

        # --- ë‚´ë¶€ ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ ---
        y_prob = best_model.predict_proba(X_test)[:, 1]
        y_pred = best_model.predict(X_test)
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        auc_score = roc_auc_score(y_test, y_prob)

        internal_results.append({
            'Model': name,
            'In_AUC': auc_score,
            'In_F1': f1_score(y_test, y_pred),
            'In_Sens': recall_score(y_test, y_pred),
            'In_Spec': tn / (tn + fp),
            'Best_Params': str(best_params)
        })

        # --- ë¶€ë“œëŸ¬ìš´ ROC ê³¡ì„  ì‹œê°í™” (Spline Interpolation) ---
        fpr, tpr, _ = roc_curve(y_test, y_prob)

        # ë³´ê°„ì„ ìœ„í•´ ì¤‘ë³µê°’ ì œê±°
        fpr_unique, indices = np.unique(fpr, return_index=True)
        tpr_unique = tpr[indices]

        # 60ê°œì˜ í¬ì¸íŠ¸ë¡œ íë¦„ ë‹¨ìˆœí™”
        fpr_new = np.linspace(0, 1, 60)
        spl = make_interp_spline(fpr_unique, tpr_unique, k=3)
        tpr_smooth = np.clip(spl(fpr_new), 0, 1)

        plt.plot(fpr_new, tpr_smooth,
                 label=f'{name:<10} (AUC: {auc_score:.3f})',
                 color=colors[i], linewidth=2.0, alpha=0.8)

        # --- ì™¸ë¶€ ê²€ì¦ (ìŠ¤ìœ„ì¹˜ ON ì‹œ ì‹¤í–‰) ---
        if run_external and external_file:
            try:
                ext_df = pd.read_csv(external_file)
                X_ext = ext_df.drop(columns=[col for col in exclude_cols if col in ext_df.columns] + [target_col])
                y_ext = ext_df[target_col]

                y_ext_prob = best_model.predict_proba(X_ext)[:, 1]
                y_ext_pred = best_model.predict(X_ext)
                tn_e, fp_e, fn_e, tp_e = confusion_matrix(y_ext, y_ext_pred).ravel()

                external_results.append({
                    'Model': name,
                    'Ext_AUC': roc_auc_score(y_ext, y_ext_prob),
                    'Ext_F1': f1_score(y_ext, y_ext_pred),
                    'Ext_Sens': recall_score(y_ext, y_ext_pred),
                    'Ext_Spec': tn_e / (tn_e + fp_e)
                })
            except Exception as e:
                print(f"âš ï¸ {name} ì™¸ë¶€ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 3. ë°ì´í„°í”„ë ˆì„ í†µí•© ë° ë¦¬í¬íŠ¸ ì¶œë ¥
    df_in = pd.DataFrame(internal_results)
    if run_external and external_results:
        df_ext = pd.DataFrame(external_results)
        final_report = pd.merge(df_in, df_ext, on='Model')
    else:
        final_report = df_in

    print("\n" + "=" * 110)
    print(f"ğŸ† ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í†µí•© ë³´ê³ ì„œ (AUC ê¸°ì¤€ ì •ë ¬)")
    pd.set_option('display.max_colwidth', None)
    print(final_report.sort_values(by='In_AUC', ascending=False).to_string(index=False))
    print("=" * 110)

    # 4. ê·¸ë˜í”„ ë ˆì´ì•„ì›ƒ ë§ˆë¬´ë¦¬
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1.5)
    plt.xlim([-0.01, 1.01])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12, fontweight='bold')
    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12, fontweight='bold')
    plt.title('Comparison of 10 Models: Smoothed ROC Curves', fontsize=15, fontweight='bold', pad=20)
    plt.legend(loc='lower right', fontsize=10, frameon=True, shadow=True)
    plt.grid(True, linestyle=':', alpha=0.6)
    plt.tight_layout()
    plt.show()

    return trained_models, X_test, final_report


def run_shap_analysis(model, X_test, model_name):
    """
    ìµœì  ëª¨ë¸ì— ëŒ€í•œ SHAP Feature Importance ì‹œê°í™”
    """
    print(f"\nğŸ’¡ {model_name} ëª¨ë¸ SHAP ê°€ì¤‘ì¹˜ ë¶„ì„ ì‹œì‘...")

    # íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì˜ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ë³€í™˜
    X_test_scaled = model.named_steps['scaler'].transform(X_test)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

    # ëª¨ë¸ë³„ Explainer ì„ íƒ
    try:
        if model_name in ['RF', 'DT', 'XGBoost', 'LightGBM', 'CatBoost', 'AdaBoost']:
            explainer = shap.TreeExplainer(model.named_steps['clf'])
            shap_values = explainer.shap_values(X_test_scaled_df)
        elif model_name == 'LR':
            explainer = shap.LinearExplainer(model.named_steps['clf'], X_test_scaled_df)
            shap_values = explainer.shap_values(X_test_scaled_df)
        else:
            # ì†ë„ë¥¼ ìœ„í•´ 50ê°œ ìƒ˜í”Œë§Œ ì‚¬ìš©
            explainer = shap.KernelExplainer(model.named_steps['clf'].predict_proba, shap.sample(X_test_scaled_df, 50))
            shap_values = explainer.shap_values(shap.sample(X_test_scaled_df, 50))
            X_test_scaled_df = shap.sample(X_test_scaled_df, 50)

        # ê²°ê³¼ ì°¨ì› ë³´ì •
        if isinstance(shap_values, list):
            shap_to_plot = shap_values[1]
        elif len(shap_values.shape) == 3:
            shap_to_plot = shap_values[:, :, 1]
        else:
            shap_to_plot = shap_values

        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_to_plot, X_test_scaled_df, plot_type="bar", show=False)
        plt.title(f"Feature Importance (SHAP) - {model_name}")
        plt.show()
    except Exception as e:
        print(f"âš ï¸ SHAP ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ==========================================
# âš™ï¸ ë©”ì¸ ì‹¤í–‰ ì„¤ì • (ì‚¬ìš©ì ë§ì¶¤ ìˆ˜ì • ê°€ëŠ¥)
# ==========================================
# 1. íŒŒì¼ ë° ì»¬ëŸ¼ ì„¤ì •
DATA_PATH = 'mimic_ver_1_0.csv'
EXT_PATH = 'external_mimic_test.csv'  # ì™¸ë¶€ íŒŒì¼ì´ ìˆì„ ê²½ìš° ì§€ì •
EXCLUDE_LIST = ['subject_id', 'hadm_id', 'stay_id']
TARGET_COL = 'outcome_icu_exit_3d'

# 2. ì‹¤í–‰ ëª¨ë¸ ì„ íƒ (ì›í•˜ëŠ” ëª¨ë¸ë§Œ ë¦¬ìŠ¤íŠ¸ì— ë„£ìœ¼ì„¸ìš”. ì „ë¶€ ì‹¤í–‰í•˜ë ¤ë©´ None)
MY_MODELS = ['LR', 'RF', 'DT', 'KNN', 'MLP', 'AdaBoost', 'SVM', 'XGBoost', 'LightGBM', 'CatBoost']
# MY_MODELS = ['XGBoost', 'LightGBM']

# 3. ì˜µì…˜ ì œì–´
USE_EXTERNAL = False  # ì™¸ë¶€ ê²€ì¦ ìˆ˜í–‰ ì—¬ë¶€

# 4. ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
models, x_test_data, report_df = run_comprehensive_analysis(
    DATA_PATH, EXCLUDE_LIST, TARGET_COL,
    run_external=USE_EXTERNAL,
    external_file=EXT_PATH,
    selected_models=MY_MODELS
)

# 5. ë¶„ì„ì´ ì„±ê³µí–ˆë‹¤ë©´ ì„±ëŠ¥ 1ìœ„ ëª¨ë¸ì— ëŒ€í•´ SHAP ë¶„ì„ ìˆ˜í–‰
if report_df is not None and not report_df.empty:
    top_model_name = report_df.sort_values(by='In_AUC', ascending=False).iloc[0]['Model']
    run_shap_analysis(models[top_model_name], x_test_data, top_model_name)