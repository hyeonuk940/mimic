import joblib
import pandas as pd
import shap
import matplotlib.pyplot as plt

def analyze_shap_logistic(model_path, data_path, exclude_cols, target_col):
    
    # 1. ì €ì¥ëœ ëª¨ë¸(GridSearchCV) ë¶ˆëŸ¬ì˜¤ê¸°
    print(f"ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘... ({model_path})")
    try:
        loaded_object = joblib.load(model_path)
    except FileNotFoundError:
        print("âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ìµœì ì˜ íŒŒì´í”„ë¼ì¸ ì¶”ì¶œ (GridSearchCV -> Pipeline)
    # ì €ì¥ëœ ê°ì²´ê°€ GridSearchë¼ë©´ .best_estimator_ë¥¼ êº¼ë‚´ì•¼ ì§„ì§œ ëª¨ë¸ì…ë‹ˆë‹¤.
    if hasattr(loaded_object, 'best_estimator_'):
        best_pipeline = loaded_object.best_estimator_
    else:
        best_pipeline = loaded_object # GridSearch ì•ˆ ì¼ìœ¼ë©´ ë°”ë¡œ íŒŒì´í”„ë¼ì¸

    # 3. íŒŒì´í”„ë¼ì¸ì—ì„œ 'ìŠ¤ì¼€ì¼ëŸ¬'ì™€ 'ë¡œì§€ìŠ¤í‹±ëª¨ë¸' ë¶„ë¦¬
    # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì´ë¦„: [('scaler', StandardScaler), ('clf', LogisticRegression)]
    scaler = best_pipeline.named_steps['scaler']
    model = best_pipeline.named_steps['clf']
    
    print("âœ… ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¶„ë¦¬ ì™„ë£Œ!")

    # 4. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (í•™ìŠµ ë•Œì™€ ë˜‘ê°™ì´!)
    df = pd.read_csv(data_path)
    
    # ì œì™¸ ì—´ ì‚­ì œ
    existing_exclude = [c for c in exclude_cols if c in df.columns]
    df = df.drop(columns=existing_exclude)
    
    # X, y ë¶„ë¦¬
    if target_col in df.columns:
        X = df.drop(columns=[target_col])
    else:
        X = df # íƒ€ê²Ÿì´ ì—†ëŠ” ìƒˆ ë°ì´í„°ì¼ ê²½ìš°
    
    # ìˆ«ì ë°ì´í„°ë§Œ ë‚¨ê¸°ê¸°
    X = X.select_dtypes(include=['number'])

    # 5. [í•µì‹¬] SHAPì„ ìœ„í•´ ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§ (Transform)
    # ëª¨ë¸ì´ í•™ìŠµí•  ë•Œ ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ë´¤ê¸° ë•Œë¬¸ì—, ì„¤ëª…í•  ë•Œë„ ì •ê·œí™”í•´ì„œ ì¤˜ì•¼ í•¨
    print("ğŸ¤– ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§ ë³€í™˜ ì¤‘...")
    X_scaled = scaler.transform(X)
    
    # ìŠ¤ì¼€ì¼ë§í•˜ë©´ ì»¬ëŸ¼ ì´ë¦„ì´ ì‚¬ë¼ì§€ë¯€ë¡œ ë‹¤ì‹œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³µêµ¬ (ê·¸ë˜í”„ì— ì´ë¦„ ë„ìš°ê¸° ìœ„í•¨)
    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

    # 6. SHAP ê°’ ê³„ì‚°
    print("ğŸ“Š SHAP ê°’ ê³„ì‚° ì‹œì‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    
    # ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” LinearExplainerê°€ ê°€ì¥ ë¹ ë¥´ê³  ì •í™•í•¨
    explainer = shap.LinearExplainer(model, X_scaled_df, feature_perturbation="interventional")
    shap_values = explainer.shap_values(X_scaled_df)

    # 7. ì‹œê°í™” 1: ìš”ì•½ ì°¨íŠ¸ (Beeswarm Plot) - ê°€ì¥ ë§ì´ ì”€
    print("\nğŸ“ˆ [1] SHAP ìš”ì•½ ì°¨íŠ¸ (Summary Plot)")
    plt.figure()
    shap.summary_plot(shap_values, X_scaled_df, show=False)
    plt.title("SHAP Summary Plot (Feature Importance)", fontsize=15)
    plt.tight_layout()
    plt.show()

    # 8. ì‹œê°í™” 2: ë§‰ëŒ€ ì°¨íŠ¸ (ì ˆëŒ€ì  ì¤‘ìš”ë„ ìˆœìœ„)
    print("\nğŸ“Š [2] ì¤‘ìš”ë„ ìˆœìœ„ ì°¨íŠ¸ (Bar Plot)")
    plt.figure()
    shap.summary_plot(shap_values, X_scaled_df, plot_type="bar", show=False)
    plt.title("Feature Importance Ranking", fontsize=15)
    plt.tight_layout()
    plt.show()

# ==========================================
# ì‹¤í–‰ ì„¤ì •
# ==========================================

# ì €ì¥í–ˆë˜ ëª¨ë¸ íŒŒì¼ëª…
saved_model = 'LR_model_1.pkl'

# ë°ì´í„° íŒŒì¼
data_file = 'mimic_ver_1_0.csv'

# ì œì™¸í•  ì»¬ëŸ¼ & íƒ€ê²Ÿ (í•™ìŠµ ë•Œë‘ ë˜‘ê°™ì´)
ex_cols = ['subject_id', 'hadm_id', 'stay_id']
target = 'outcome_icu_exit_3d'

# ì‹¤í–‰
analyze_shap_logistic(saved_model, data_file, ex_cols, target)